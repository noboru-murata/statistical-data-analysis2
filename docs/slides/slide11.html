<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>クラスタ分析</title>
<meta name="author" content="村田 昇"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="./reveal.js/dist/reveal.css"/>

<link rel="stylesheet" href="./reveal.js/dist/theme/oer-reveal.css" id="theme"/>

<link rel="stylesheet" href="./reveal.js/plugin/toc-progress/toc-progress.css"/>

<link rel="stylesheet" href="./reveal.js/dist/theme/toc-style.css"/>

<link rel="stylesheet" href="./reveal.js/dist/theme/fonts/source-sans-pro/source-sans-pro.css"/>

<link rel="stylesheet" href="./reveal.js/plugin/accessibility/helper.css"/>

<link rel="stylesheet" href="./reveal.js/dist/theme/mycourse.css"/>
<link rel="stylesheet" href="./reveal.js/plugin/highlight/zenburn.css"/>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide" data-state="no-toc-progress">
<!-- This is an HTML template for the title slide. -->
<!-- Embed logos as necessary. -->
<!-- <a class="nooutlink" href="url"><img class="state-background your-logo-class" src="whatever.png" alt="Whatever" /></a> -->
<div class="talk-title">
    <h1 class="no-toc-progress">クラスタ分析</h1>
</div>
<div class="talk-subtitle">
    <p>非階層的方法と分析の評価</p>
</div>
<div class="keyboard-usage">
    <p>(Press <code>?</code> for help, <code>n</code> and <code>p</code> for next and previous slide)</p>
</div>
<div class="talk-author">
  <p>村田 昇<br />
  </p>
</div>

</section>

<section>
<section id="slide-org795b1b0">
<h2 id="org795b1b0">講義概要</h2>
<ul>
<li>第1日: クラスタ分析の考え方と階層的方法</li>
<li><b>第2日: 非階層的方法と分析の評価</b></li>

</ul>
<div class="slide-footer"><br></div>

</section>
</section>
<section>
<section id="slide-org6f31029">
<h2 id="org6f31029">クラスタ分析の復習</h2>
<div class="outline-text-2" id="text-org6f31029">
</div>
</section>
<section id="slide-orga73c99d">
<h3 id="orga73c99d">クラスタ分析</h3>
<ul>
<li><p>
<b>cluster analysis</b>
</p>
<blockquote>
<p>
個体の間に隠れている
<b>集まり=クラスタ</b>
を個体間の&ldquo;距離&rdquo;にもとづいて発見する方法
</p>
</blockquote></li>
<li>個体間の類似度・距離(非類似度)を定義 :
<ul>
<li>同じクラスタに属する個体どうしは似通った性質</li>
<li>異なるクラスタに属する個体どうしは異なる性質</li>

</ul></li>
<li>さらなるデータ解析やデータの可視化に利用</li>
<li>教師なし学習の代表的な手法の一つ</li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-orgd5234ff">
<h3 id="orgd5234ff">クラスタ分析の考え方</h3>
<ul>
<li>階層的方法 :
<ul>
<li>データ点およびクラスタの間に <b>距離</b> を定義</li>
<li>距離に基づいてグループ化:
<ul>
<li>近いものから順にクラスタを <b>凝集</b></li>
<li>近いものが同じクラスタに残るように <b>分割</b></li>

</ul></li>

</ul></li>
<li>非階層的方法 :
<ul>
<li>クラスタの数を事前に指定</li>
<li>クラスタの <b>集まりの良さ</b> を評価する損失関数を定義</li>
<li>損失関数を最小化するようにクラスタを形成</li>

</ul></li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-org4149925">
<h3 id="org4149925">階層的クラスタリング</h3>
<div class="leftcol" id="orgd8d95a9">
<ul>
<li>凝集的手続き
<ol>
<li>データ・クラスタ間の距離を定義
<ul>
<li>データ点間の距離</li>
<li>クラスタ間の距離</li>

</ul></li>
<li>データ点およびクラスタ間の距離を計算</li>
<li>最も近い2つを統合し新たなクラスタを形成</li>
<li>クラスタ数が1つになるまで2-3の手続きを繰り返す</li>

</ol></li>

</ul>

</div>
<div class="rightcol" id="org8b8c9e0">

<div id="org5e7f1bb" class="figure">
<p><img src="figs/11_hclst.png" alt="11_hclst.png" />
</p>
<p><span class="figure-number">Figure 1: </span>凝集的手続きの例</p>
</div>

</div>
<div class="slide-footer"><br></div>


</section>
</section>
<section>
<section id="slide-orge205d23">
<h2 id="orge205d23">非階層的方法</h2>
<div class="outline-text-2" id="text-orge205d23">
</div>
</section>
<section id="slide-orgfc5a895">
<h3 id="orgfc5a895">非階層的方法の手続き</h3>
<ul>
<li>対象の変数 : \(\boldsymbol{X}=(X_1,X_2,\dotsc,X_{d})^{\mathsf{T}}\) (\(d\)次元)</li>
<li><p>
観測データ : \(n\) 個の個体の組
</p>
<blockquote>
<div>
\begin{equation}
  \{\boldsymbol{x}_{i}\}_{i=1}^{n}
  =
  \{(x_{i1},x_{i2},\dotsc,x_{id})^{\mathsf{T}}\}_{i=1}^{n}
\end{equation}

</div>
</blockquote></li>
<li><p>
個体とクラスタの対応 \(C\) を推定 :
</p>
<blockquote>
<div>
\begin{equation}
  C(i)
  =\text{(個体 \(i\) が属するクラスタ番号)}
\end{equation}

</div>
</blockquote>
<ul>
<li>対応 \(C\) の <b>全体の良さ</b> を評価する損失関数を設定</li>
<li>観測データ
\(\{\boldsymbol{x}_{i}\}_{i=1}^{n}\)
に最適な対応
\(\{C(i)\}_{i=1}^{n}\) を決定</li>

</ul></li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-org341b1db">
<h3 id="org341b1db">\(k\)-平均法の損失関数</h3>
<ul>
<li>クラスタの個数 \(k\) を指定</li>
<li><p>
2つの個体 \(i,i'\) の <b>近さ=損失</b> を距離の二乗で評価
</p>
<blockquote>
<div>
\begin{equation}
  \|\boldsymbol{x}_i-\boldsymbol{x}_{i'}\|^2
  =
  \sum_{j=1}^{d}(x_{ij}-x_{i'j})^2
\end{equation}

</div>
</blockquote></li>
<li><p>
損失関数 \(W(C)\) : クラスタ内の平均の近さを評価
</p>
<blockquote>
<div>
\begin{equation}
  W(C)
  =
  \sum_{l=1}^k\frac{1}{n_l}\sum_{i:C(i)=l}\sum_{i':C(i')=l}\|\boldsymbol{x}_i-\boldsymbol{x}_{i'}\|^2
\end{equation}

</div>
</blockquote></li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-orgf84558d">
<h3 id="orgf84558d">\(k\)-平均法の性質</h3>
<ul>
<li><p>
クラスタ \(l\) に属する個体の平均 :
</p>
<blockquote>
<div>
\begin{equation}
  \bar{\boldsymbol{x}}_l
  =
  \frac{1}{n_l}\sum_{i:C(i)=l}\boldsymbol{x}_i
\end{equation}

</div>
</blockquote></li>
<li><p>
損失関数 \(W(C)\) の等価な表現 :
</p>
<blockquote>
<div>
\begin{equation}
  W(C)
  =
  2\sum_{l=1}^k\sum_{i:C(i)=l}\|\boldsymbol{x}_i-\bar{\boldsymbol{x}}_{l}\|^2
\end{equation}

</div>
</blockquote></li>
<li>最適な対応 \(C\) : クラスタ内変動の総和が最小</li>

</ul>
<div class="slide-footer"><br></div>


</section>
</section>
<section>
<section id="slide-org7aa5af3">
<h2 id="org7aa5af3">近似的な最適化</h2>
<div class="outline-text-2" id="text-org7aa5af3">
</div>
</section>
<section id="slide-orgd508b06">
<h3 id="orgd508b06">クラスタ対応の最適化</h3>
<ul>
<li>最適化: 損失関数 \(W(C)\) を最小とする \(C\) を決定</li>
<li>貪欲な \(C\) の探索:
<ul>
<li>原理的には全ての値を計算すればよい</li>
<li>可能な \(C\) の数: \(k^n\) 通り (有限個のパターン)</li>
<li>サンプル数 \(n\) が小さくない限り実時間での実行は不可能</li>

</ul></li>
<li>近似的な \(C\) の探索:
<ul>
<li>いくつかのアルゴリズムが提案されている</li>
<li><p>
基本的な考え方: <b>Lloyd-Forgyのアルゴリズム</b>
</p>
<blockquote>
<p>
標本平均と変動の平方和の性質を利用
</p>
<div>
\begin{equation}
  \bar{\boldsymbol{x}}_l
  =\arg\min_{\mu}
  \sum_{i:C(i)=l}\|\boldsymbol{x}_i-\boldsymbol{\mu}\|^2
  \quad
  \text{(クラスタ\(l\)の標本平均)}
\end{equation}

</div>
</blockquote></li>

</ul></li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-org9085186">
<h3 id="org9085186">Lloyd-Forgyのアルゴリズム</h3>
<ol>
<li>クラスタ中心の初期値 
\(\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\dots,\boldsymbol{\mu}_k\) を与える</li>
<li><p>
各データの所属クラスタ番号 \(C(i)\) を求める
</p>
<blockquote>
<div>
\begin{equation}
  C(i)
  =
  \arg\min_l\|\boldsymbol{x}_i-\boldsymbol{\mu}_l\|
\end{equation}

</div>
</blockquote></li>
<li><p>
各クラスタ中心 \(\boldsymbol{\mu}_l\;(l=1,2,\dotsc,k)\) を更新する
</p>
<blockquote>
<div>
\begin{equation}
  \boldsymbol{\mu}_l
  =
  \frac{1}{n_l}\sum_{i:C(i)=l}\boldsymbol{x}_i,
  \quad
  n_l=|\{\boldsymbol{x}_i|C(i)=l\}|
\end{equation}

</div>
</blockquote></li>
<li>中心が変化しなくなるまで 2,3 を繰り返す</li>

</ol>
<div class="slide-footer"><br></div>

</section>
<section id="slide-org80f8ab7">
<h3 id="org80f8ab7">アルゴリズムの性質</h3>
<ul>
<li>結果は <b>確率的</b>
<ul>
<li>初期値 \(\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\dots,\boldsymbol{\mu}_k\) に依存</li>
<li>アルゴリズムの成否は確率的 <br />
(最適解が得られない場合もある)</li>

</ul></li>
<li>一般には複数の初期値をランダムに試して損失を最小とする解を採用する</li>
<li><p>
平均の代わりにメドイド (medoid; 中心にある観測値) を用いる方法もある
</p>
<blockquote>
<div>
\begin{equation}
  \boldsymbol{x}^{\mathrm{medoid}}_{l}
  =\arg\min_{\boldsymbol{x}_{i}}
  \sum_{i':C(i')=l}
  \|\boldsymbol{x}_{i}-\boldsymbol{x}_{i'}\|^2
\end{equation}

</div>
</blockquote></li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-orgcb493b6">
<h3 id="orgcb493b6">事例</h3>
<ul>
<li><p>
都道府県別好きなおむすびの具(一部)での例
</p></li>

</ul>


<div id="org1b02d35" class="figure">
<p><img src="figs/11_nhclst0.png" alt="11_nhclst0.png" />
</p>
<p><span class="figure-number">Figure 2: </span>非階層的クラスタリング</p>
</div>

<div class="slide-footer"><br></div>
</section>
<section>

<div id="orgd7fce6b" class="figure">
<p><img src="figs/11_nhclst1.png" alt="11_nhclst1.png" />
</p>
<p><span class="figure-number">Figure 3: </span>Lloyd-Forgyのアルゴリズム (その1)</p>
</div>

<div class="slide-footer"><br></div>
</section>
<section>

<div id="org8f264a4" class="figure">
<p><img src="figs/11_nhclst2.png" alt="11_nhclst2.png" />
</p>
<p><span class="figure-number">Figure 4: </span>Lloyd-Forgyのアルゴリズム (その2)</p>
</div>

<div class="slide-footer"><br></div>
</section>
<section>

<div id="orgcd5caa0" class="figure">
<p><img src="figs/11_nhclst3.png" alt="11_nhclst3.png" />
</p>
<p><span class="figure-number">Figure 5: </span>Lloyd-Forgyのアルゴリズム (その3)</p>
</div>

<div class="slide-footer"><br></div>
</section>
<section>

<div id="orgeefb912" class="figure">
<p><img src="figs/11_nhclst4.png" alt="11_nhclst4.png" />
</p>
<p><span class="figure-number">Figure 6: </span>Lloyd-Forgyのアルゴリズム (その4)</p>
</div>

<div class="slide-footer"><br></div>
</section>
<section>

<div id="orga4c3d68" class="figure">
<p><img src="figs/11_nhclst5.png" alt="11_nhclst5.png" />
</p>
<p><span class="figure-number">Figure 7: </span>Lloyd-Forgyのアルゴリズム (その5)</p>
</div>

<div class="slide-footer"><br></div>
</section>
<section>

<div id="orgbb73c0e" class="figure">
<p><img src="figs/11_nhclst6.png" alt="11_nhclst6.png" />
</p>
<p><span class="figure-number">Figure 8: </span>Lloyd-Forgyのアルゴリズム (その6)</p>
</div>

<div class="slide-footer"><br></div>
</section>
<section>

<div id="org9b39562" class="figure">
<p><img src="figs/11_nhclst.png" alt="11_nhclst.png" />
</p>
<p><span class="figure-number">Figure 9: </span>クラスタリングの結果</p>
</div>
<div class="slide-footer"><br></div>


</section>
</section>
<section>
<section id="slide-orgdf5016b" data-background="#fef4f4">
<h2 id="orgdf5016b">演習</h2>
<div class="slide-footer"><br></div>
</section>
<section id="slide-orge038f13" data-background="#fef4f4">
<h3 id="orge038f13">R: 関数 <code>kmeans()</code></h3>
<ul>
<li><p>
\(k\)-平均法を実行するための標準的な関数
</p>
<div class="org-src-container">

<pre><code class="R" >kmeans(x, centers, iter.max = 10, nstart = 1,
       algorithm = "Hartigan-Wong")
## x: データフレーム
## centers: クラスタ数
## iter.max: 最大繰り返し数
## nstart: 初期値の候補数
## algorithm: 最適化法の指定．他に "Lloyd", "Forgy", "MacQueen" が指定可
</code></pre>
</div></li>
<li>結果は変数のスケールにも依存
<ul>
<li>例えば測定値の単位により異なる</li>
<li>必要ならば主成分分析の場合と同様にデータの標準化を行う</li>

</ul></li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-org2fc6cc1" data-background="#fef4f4">
<h3 id="org2fc6cc1">練習問題</h3>
<ul>
<li><p>
データセット <code>japan_social.csv</code> を用いて
以下を確認しなさい
</p>
<div class="org-src-container">

<pre><code class="R" >JS.data &lt;- read.csv("data/japan_social.csv", row.names=1)
</code></pre>
</div>
<ul>
<li>関数 <code>kmeans()</code> を用いて
各変数平均0，分散1に標準化
(関数 <code>scale()</code> を利用)
したデータを7クラスタに分割しなさい</li>
<li>各クラスタ内の県名を表示しなさい</li>
<li>関数 <code>clusplot()</code> を用いて
2次元散布図に各クラスタを表示しなさい</li>

</ul></li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-orgeea07da" data-background="#fef4f4">
<h3 id="orgeea07da">R: 関数 <code>cluster::pam()</code></h3>
<ul>
<li><p>
\(k\)-メドイド法を実行するための関数
</p>
<div class="org-src-container">

<pre><code class="R" >pam(x, k, metric = "euclidean", stand = FALSE)
## x: データフレーム，または距離行列 
## k: クラスタの数
## metric: 距離の指定(xがデータフレームの場合)．他に "manhattan" が指定可
## stand: 標準化(平均0，絶対偏差1)
</code></pre>
</div>
<ul>
<li>詳細は <code>?cluster::pam</code> を参照</li>

</ul></li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-org2727afb" data-background="#fef4f4">
<h3 id="org2727afb">練習問題</h3>
<ul>
<li>データセット <code>japan_social.csv</code> を用いて
以下を確認しなさい
<ul>
<li>関数 <code>pam()</code> を用いて
各変数平均0，絶対偏差1に標準化したデータを7クラスタに分割しなさい</li>
<li>各クラスタ内の県名を表示しなさい</li>
<li>関数 <code>clusplot()</code> を用いて
2次元散布図に各クラスタを表示しなさい</li>

</ul></li>
<li>余裕がある人はデータセット <code>omusubi.csv</code> でも確認しなさい</li>

</ul>
<div class="slide-footer"><br></div>

</section>
</section>
<section>
<section id="slide-orge7d4121">
<h2 id="orge7d4121">クラスタ構造の評価</h2>
<div class="outline-text-2" id="text-orge7d4121">
</div>
</section>
<section id="slide-orga2113fe">
<h3 id="orga2113fe">階層的方法の評価</h3>
<ul>
<li>評価の対象
<ul>
<li><p>
データ \(\boldsymbol{x}_i\) と最初に統合されたクラスタ \(C\) の距離:
</p>
<blockquote>
<div>
\begin{equation}
  d_i
  =
  D({\boldsymbol{x}_i},C)
\end{equation}

</div>
</blockquote></li>
<li><p>
最後に統合された2つのクラスタ \(C',C''\) の距離:
</p>
<blockquote>
<div>
\begin{equation}
  D
  =
  D(C',C'')
\end{equation}

</div>
</blockquote></li>

</ul></li>
<li><p>
<b>凝集係数</b> (agglomerative coefficient):
</p>
<blockquote>
<div>
\begin{equation}
  AC
  =
  \frac{1}{n}\sum_{i=1}^{n}\left(1-\frac{d_i}{D}\right)
\end{equation}

</div>
</blockquote></li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-org0959ed1">
<h3 id="org0959ed1">凝集係数の性質</h3>
<ul>
<li><p>
定義より
</p>
<blockquote>
<div>
\begin{equation}
  0\le AC\le 1
\end{equation}

</div>
</blockquote></li>
<li>1に近いほどクラスタ構造が明瞭</li>
<li>banner plot: 各 \((1-{d_i}/{D})\) を並べた棒グラフ</li>
<li>banner plot の面積比として視覚化</li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-orgfe4a37b">
<h3 id="orgfe4a37b">非階層的方法の評価</h3>
<ul>
<li>評価の対象
<ul>
<li><p>
\(\boldsymbol{x}_i\) を含むクラスタ \(C^1\) と \(\boldsymbol{x}_i\) の距離:
</p>
<blockquote>
<div>
\begin{equation}
  d^1_i=D({\boldsymbol{x}_i},C^1\setminus{\boldsymbol{x}_i})
\end{equation}

</div>
</blockquote></li>
<li><p>
一番近いクラスタ \(C^2\) と \(\boldsymbol{x}_i\) の距離:
</p>
<blockquote>
<div>
\begin{equation}
  d^2_i=D({\boldsymbol{x}_i},C^2)
\end{equation}

</div>
</blockquote></li>

</ul></li>
<li><p>
<b>シルエット係数</b> (silhouette coefficient):
</p>
<blockquote>
<div>
\begin{equation}
  S_i
  =
  \frac{d^2_i-d^1_i}{\max(d^1_i,d^2_i)}
\end{equation}

</div>
</blockquote></li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-orgd2ebe28">
<h3 id="orgd2ebe28">シルエット係数の性質</h3>
<ul>
<li><p>
定義より
</p>
<blockquote>
<div>
\begin{equation}
  -1\le S_i\le 1
\end{equation}

</div>
</blockquote></li>
<li>1に近いほど適切なクラスタリング</li>
<li>全体の良さを評価するには \(S_i\) の平均を用いる</li>
<li>距離の計算を適切に行えば階層的方法でも利用可</li>

</ul>
<div class="slide-footer"><br></div>


</section>
</section>
<section>
<section id="slide-org07aa8de" data-background="#fef4f4">
<h2 id="org07aa8de">演習</h2>
<div class="slide-footer"><br></div>
</section>
<section id="slide-org3be4001" data-background="#fef4f4">
<h3 id="org3be4001">R: 関数 <code>cluster::agnes()</code></h3>
<ul>
<li><p>
関数 <code>agnes()</code> による凝集係数の取得
</p>
<div class="org-src-container">

<pre><code class="R" >### 凝集係数の取得
summary(agnes(x, # データフレーム
	      stand = TRUE, # 正規化
	      metric = "euclidean", # ユークリッド距離
	      method = "average") # 群平均法
	)$ac 
### 凝集係数の視覚化 (banner plot)
plot(agnes(x), # 階層的クラスタリングの結果
     which.plots=1) # banner plot を選択
## plot および cluster::bannerplot のオプションを参考
</code></pre>
</div></li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-orgb2bd76c" data-background="#fef4f4">
<h3 id="orgb2bd76c">練習問題</h3>
<ul>
<li>データセット <code>japan_social.csv</code> を用いて
以下を検討しなさい
<ul>
<li>関数 <code>agnes()</code> を用いて階層的クラスタリングを行いなさい
<ul>
<li>標準化: 行う</li>
<li>データ距離: ユークリッド距離，およびマンハッタン距離</li>
<li>クラスタ距離: 群平均法</li>

</ul></li>
<li>凝集係数を用いて2つの距離の評価を行いなさい</li>

</ul></li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-orgbe71c94" data-background="#fef4f4">
<h3 id="orgbe71c94">R: 関数 <code>cluster::pam()</code></h3>
<ul>
<li><p>
関数 <code>pam()</code> によるシルエット係数の取得
</p>
<div class="org-src-container">

<pre><code class="R" >### シルエット係数関連の情報取得
summary(pam(x, k))$silinfo
### 各データのシルエット係数
summary(pam(x, k))$silinfo$widths
### 各クラスタのシルエット係数の平均
summary(pam(x, k))$silinfo$clus.avg.widths
### シルエット係数の平均
summary(pam(OM.dsy,k=k))$silinfo$avg.width
### シルエット係数の視覚化 (silhouette plot)
plot(pam(OM.dsy,k=k), which.plot=2)
## plot および cluster::silhouette のオプションを参考
</code></pre>
</div></li>

</ul>
<div class="slide-footer"><br></div>

</section>
<section id="slide-org323282c" data-background="#fef4f4">
<h3 id="org323282c">練習問題</h3>
<ul>
<li>データセット <code>omusubi.csv</code> を用いて
以下を検討しなさい
<ul>
<li><p>
Hellinger距離を用いて距離行列を計算しなさい
</p>
<blockquote>
<p>
\(\boldsymbol{p},\boldsymbol{q}\)
を確率ベクトルとして
定義される確率分布の距離
</p>
<div>
\begin{equation}
  d_{\mathrm{hel}}(\boldsymbol{p},\boldsymbol{q})
  =
  \frac{1}{\sqrt{2}}d_{\mathrm{euc}}(\sqrt{\boldsymbol{p}},\sqrt{\boldsymbol{q}})
\end{equation}

</div>
</blockquote></li>
<li>クラスタ数4-10のシルエット係数を比較しなさい</li>
<li>適当と思われるクラスタ数による分析を行いなさい</li>

</ul></li>

</ul>
<div class="slide-footer"><br></div>

</section>
</section>
<section>
<section id="slide-orgd995df9">
<h2 id="orgd995df9">次週の予定</h2>
<ul>
<li><b>第1日: 時系列の基本モデル</b></li>
<li>第2日: モデルの推定と予測</li>

</ul>
<div class="slide-footer"><br></div>
</section>
</section>
</div>
</div>
<script src="./reveal.js/dist/reveal.js"></script>
<script src="./reveal.js/plugin/notes/notes.js"></script>
<script src="./reveal.js/plugin/search/search.js"></script>
<script src="./reveal.js/plugin/zoom/zoom.js"></script>
<script src="./reveal.js/plugin/highlight/highlight.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: true,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: true,
hashOneBasedIndex: false,
pdfSeparateFragments: false,
overview: true,

transition: 'fade',
transitionSpeed: 'default',
spotlight: { size: 90, initialPresentationMode: false }, chalkboard: { toggleChalkboardButton: { left: '80px' }, toggleNotesButton: { left: '130px'}}, keyboard: { 69: function() { RevealChalkboard.toggleNotesCanvas() }, 87: function() { RevealChalkboard.toggleChalkboard() }, 67: function() { RevealChalkboard.clear() }, 82: function() { RevealChalkboard.reset() }, 68: function() { RevealChalkboard.download() }, 88: function() { RevealChalkboard.colorNext() }, 89: function() { RevealChalkboard.colorPrev() }, 84: function() { RevealSpotlight.toggleSpotlight() }, 81: function() { RevealSpotlight.togglePresentationMode()}},

// Plugins with reveal.js 4.x
plugins: [ RevealNotes, RevealSearch, RevealZoom, RevealHighlight ],

// Optional libraries used to extend reveal.js
dependencies: [
{ src: './reveal.js/plugin/menu/menu.js'},
{ src: './reveal.js/plugin/chalkboard/chalkboard.js'},
{ src: './reveal.js/plugin/spotlight/spotlight.js'}]

});
</script>
</body>
</html>
